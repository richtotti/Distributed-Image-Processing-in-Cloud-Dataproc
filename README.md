# Distributed-Image-Processing-in-Cloud-Dataproc
 Hands-on lab in GCP using Apache Spark on Cloud Dataproc to distribute a computationally intensive image processing task onto a cluster of machines.
# Set Up
# Create a development machine in  compute engine 
First is to create a virtual machine to host your services.
# Install Software
Now set up the software to run the job
# Create a GCS bucket and collect images
Now that you built your Feature Detector files, create a Cloud Storage bucket and add some sample images to it.
# Submit your job to Cloud Dataproc
